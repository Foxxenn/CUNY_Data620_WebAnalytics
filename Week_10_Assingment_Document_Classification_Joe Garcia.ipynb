{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b182c637",
   "metadata": {},
   "source": [
    "# Joe Garcia\n",
    "# 12/16/25\n",
    "# Data 620 Web Analytics\n",
    "# Week_10_Assignment_Document_Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ad427",
   "metadata": {},
   "source": [
    "This project uses the Spambase dataset to build a model that can classify emails as spam or non-spam. The goal is to train a simple but effective classifier using numeric email features, then evaluate it in a fair way by splitting the data into training, dev, and test sets. To keep the workflow consistent, we use a scikit-learn pipeline that standardizes the features and fits a Logistic Regression model, then report performance using a confusion matrix and classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94079af7",
   "metadata": {},
   "source": [
    "# Loading the Spambase Dataset and Splitting Features vs. Labels\n",
    "\n",
    "This code pulls the Spambase dataset directly from the UCI Machine Learning Repository and loads it into a pandas DataFrame. Since the file doesn’t include column names, it’s read with header=None, so the columns are numbered automatically. Then the dataset is split into inputs and outputs: X contains the first 57 columns, which are the numeric features used to describe each email, and y is the last column, which is the class label—1 means spam and 0 means non-spam (ham)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1e8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59dadfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\" \n",
    "df = pd.read_csv(url, header=None) \n",
    "\n",
    "X = df.iloc[:, :-1] # 57 numeric features \n",
    "y = df.iloc[:, -1] # class label (1=spam, 0=non-spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77c608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "757f7110",
   "metadata": {},
   "source": [
    "# Splitting the Data into Train, Dev, and Test Sets\n",
    "\n",
    "This section splits the dataset into three parts so we can train the model, tune it, and then evaluate it fairly. First, train_test_split separates out 60% of the data for training and keeps the remaining 40% in a temporary set. Then that temporary set is split again into 20% dev and 20% test (each half of the 40%). The stratify= option is used in both splits so the spam vs. non-spam proportions stay consistent across all three sets, and random_state=42 ensures the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908b0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.40, random_state=42, stratify=y\n",
    ")\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141908f1",
   "metadata": {},
   "source": [
    "# Building and Evaluating a Logistic Regression Pipeline on the Dev Set\n",
    "\n",
    "\n",
    "Here, a scikit-learn Pipeline is created to keep preprocessing and modeling in one clean workflow. The StandardScaler() step standardizes all 57 numeric features so they’re on a similar scale, which helps Logistic Regression train more reliably. The classifier is a LogisticRegression model with max_iter=2000 to give it enough iterations to converge. After fitting the pipeline on the training data, the model is tested on the dev set (the set used during development) and printed results include a confusion matrix plus a classification report showing precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35b0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV SET RESULTS\n",
      "[[535  23]\n",
      " [ 45 317]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.922     0.959     0.940       558\n",
      "           1      0.932     0.876     0.903       362\n",
      "\n",
      "    accuracy                          0.926       920\n",
      "   macro avg      0.927     0.917     0.922       920\n",
      "weighted avg      0.926     0.926     0.926       920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate on DEV set (used during development) ---\n",
    "dev_pred = model.predict(X_dev)\n",
    "print(\"DEV SET RESULTS\")\n",
    "print(confusion_matrix(y_dev, dev_pred))\n",
    "print(classification_report(y_dev, dev_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9989355",
   "metadata": {},
   "source": [
    "# Final Evaluation on the Test Set\n",
    "\n",
    "After finishing development on the dev set, this section evaluates the trained pipeline on the test set, which is meant to be the “final exam” for the model. The code generates predictions for X_test, then prints a confusion matrix to show how many emails were correctly or incorrectly classified as spam vs. non-spam. It also prints a classification report with precision, recall, and F1-score, giving a clearer picture of performance than accuracy alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b7bd54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST SET RESULTS\n",
      "[[528  30]\n",
      " [ 46 317]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.920     0.946     0.933       558\n",
      "           1      0.914     0.873     0.893       363\n",
      "\n",
      "    accuracy                          0.917       921\n",
      "   macro avg      0.917     0.910     0.913       921\n",
      "weighted avg      0.917     0.917     0.917       921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "print(\"\\nTEST SET RESULTS\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(classification_report(y_test, test_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09867d2e",
   "metadata": {},
   "source": [
    "# Predicting Spam Probability for New Emails\n",
    "This section treats a few rows from the test set as “new” unseen emails and shows how the model behaves on individual examples. It selects the first five test records, then uses predict_proba() to return the model’s confidence for each class (non-spam vs. spam) and predict() to return the final predicted label. The results are organized into a small DataFrame so you can quickly compare the predicted class with the probability scores, which is useful when you want more detail than a simple 0/1 prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c8e5789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEW DOCUMENT PREDICTIONS (sample)\n",
      "   pred_label  prob_nonspam  prob_spam\n",
      "0           0      0.889214   0.110786\n",
      "1           1      0.000926   0.999074\n",
      "2           0      0.999996   0.000004\n",
      "3           0      0.930433   0.069567\n",
      "4           1      0.048542   0.951458\n"
     ]
    }
   ],
   "source": [
    "new_docs = X_test.iloc[:5]                 # these are \"new/unseen\" to the model\n",
    "new_probs = model.predict_proba(new_docs)  # probability of each class\n",
    "new_labels = model.predict(new_docs)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"pred_label\": new_labels.astype(int),\n",
    "    \"prob_nonspam\": new_probs[:, 0],\n",
    "    \"prob_spam\": new_probs[:, 1],\n",
    "})\n",
    "print(\"\\nNEW DOCUMENT PREDICTIONS (sample)\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc17d2",
   "metadata": {},
   "source": [
    "# Conclusion and Discussion\n",
    "\n",
    "In this project, we trained a Logistic Regression model to classify emails as spam or non-spam using the Spambase dataset. We split the data into training, dev, and test sets so we could improve the model using the dev set while still keeping the test set for a fair final check. Putting scaling and the classifier into a pipeline also kept the workflow consistent and clean.\n",
    "\n",
    "Overall, the dev and test results were similar, which suggests the model generalizes well. The probability outputs are also useful because they show confidence, not just a yes/no label, which can help flag uncertain emails for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb87bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
